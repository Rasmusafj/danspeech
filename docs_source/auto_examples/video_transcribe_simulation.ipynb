{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nSimulates transcribing a video with DanSpeech\n=============================================\n\nThis is an example where a full video (converted to .wav) is being transcribed as it would be transcribed\nin a setting where the input was a source with chunk size 1024 (think of a microphone).\n\nThis specific example was used to transcribe a\n`\"Udvalgsm\u00f8de\" from Folketinget (Danish Parliament) <https://www.ft.dk/aktuelt/webtv/video/20182/beu/td.1583453.aspx?as=1>`_\nwith offset 18 seconds.\n\nThe result can be seen in `Udvalgsm\u00f8de result <https://gist.github.com/Rasmusafj/fb416032f70331a5641446bb0e61d008>`_\n\nNote that the video needs to be converted to the correct format. See below:\n\n- First convert your video to .wav in the correct format\n- Example using ffmpeg:\n\n.. code-block:: bash\n\n    ffmpeg -i video.mp4 -vn -acodec pcm_s16le -ar 16000 -ac 1 video_audio.wav\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from danspeech import Recognizer\nfrom danspeech.pretrained_models import Folketinget\nfrom danspeech.language_models import Folketinget3gram\nfrom danspeech.audio import load_audio\n\nimport numpy as np\n\nimport argparse\n\n\n# First convert your video to .wav in the correct format\n# Example using ffmpeg:\n# ffmpeg -i video.mp4 -vn -acodec pcm_s16le -ar 16000 -ac 1 video_audio.wav\nparser = argparse.ArgumentParser(description='DanSpeech simulate transcribing video')\nparser.add_argument('--wav-path', type=str, required=True,  help='Path to the .wav file which you wish to transcribe')\nparser.add_argument('--gpu', action='store_true', help='Whether to use danspeech with GPU (Recommended)')\nparser.add_argument('--offset', type=int, default=0, help='Offset in seconds. Start video not from start')\nparser.add_argument('--outfile', type=str, default=\"\", required=False,\n                    help='path to write the results. (from where the script is run)')\n\n\ndef pretty_format_seconds(seconds):\n    minutes = str(int(seconds / 60))\n    remaining_seconds = str(int(seconds % 60))\n    if int(remaining_seconds) < 10:\n        remaining_seconds = \"0\" + remaining_seconds\n\n    return \"{0}:{1}\".format(minutes, remaining_seconds)\n\n\nif __name__ == '__main__':\n    args = parser.parse_args()\n    model = Folketinget()\n    lm = Folketinget3gram()\n    recognizer = Recognizer(model=model, lm=lm, with_gpu=args.gpu, alpha=1.0471119809697471,\n                            beta=2.8309374387487924, beam_width=64)\n\n    # Load audio and do an offset start\n    audio = load_audio(args.wav_path)\n    offset = args.offset * 16000\n    audio = audio[offset:]\n\n    # Energy threshold. This can vary a lot depending on input\n    energy_threshold = 600\n\n    # Simulate a stream with chunk size 1024\n    step = 1024\n    iterator = 0\n\n    # seconds of non-speaking audio before a phrase is considered complete\n    pause_threshold = 0.55\n\n    pause_buffer_count = np.ceil(pause_threshold / (1024 / 16000))\n\n    # seconds of speaking before considering it a phrase\n    phrase_threshold = 0.2\n    phrase_buffer_count = np.ceil(phrase_threshold / (1024 / 16000))\n\n    # Control variables\n    is_speaking = False\n    frames_counter = 0\n    pause_count = 0\n\n    if args.outfile:\n        f = open(args.outfile, \"w\", encoding=\"utf-8\")\n\n    # Main loop\n    while (iterator + step) < len(audio):\n\n        # Get data\n        temp_data = audio[iterator:iterator + step]\n\n        # Simple energy measure\n        energy = np.sqrt((temp_data * temp_data).sum() / (1. * len(temp_data)))\n\n        # If energy is above, then speaking has started\n        if energy > energy_threshold and not is_speaking:\n            # General requirements for start\n            is_speaking = True\n\n            # We give the previous ~0.120 seconds with the output i.e. two frames just in case.\n            start_index = iterator - 2*step\n\n            # Must not be negative though\n            if start_index < 0:\n                start_index = iterator\n\n        # add to iterator here!\n        iterator += step\n\n        if is_speaking:\n            frames_counter += 1\n\n            # Control whether we should stop\n            if energy > energy_threshold:\n                pause_count = 0\n            else:\n                pause_count += 1\n\n        # This indicaes we should stop!\n        if pause_count > pause_buffer_count and is_speaking:  # end of the phrase\n\n            # now check how long the spoken utterance was disregarding the \"not enough energy\" pause count\n            if (frames_counter - pause_count) > phrase_buffer_count:\n                trans = recognizer.recognize(audio[start_index:iterator])\n\n                # samples --> seconds --> pretty format\n                start = pretty_format_seconds((start_index + offset) / 16000)\n                end = pretty_format_seconds((iterator + offset) / 16000)\n                out_string = \"start: {0}, end: {1}, transcription: {2}\".format(start, end, trans)\n                print(out_string)\n\n                if args.outfile:\n                    f.write(out_string + \"\\n\")\n\n            is_speaking = False\n            frames_counter = 0\n            pause_count = 0\n\n    f.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}